<!doctype html>
<html>
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="initial-scale=1.0, user-scalable=no" />
	<title>Minimal Ogv.js example</title>

	<script src="lib/ogv.js?version=OGV_VERSION"></script>
</head>
<body>

<div id="videoarea"></div>

<select onchange="switchmedia(this)">
	<option value="media/llama-drama-av1.webm">Llama drama (AV1, Opus; 180p)</option>
	<option value="media/curiosity.ogv">Curiosity's Seven Minutes of Terror (Vorbis, Theora; 160p)</option>
	<option value="media/ehren-paper_lights-96.opus">Ehren - Paper Lights (Opus)</option>
	<option value="media/pixel_aspect_ratio.ogg">Theora Test suite pixel_aspect_ratio.ogg</option>
</select>
<div>
<button onclick="play(sampleFile);">Play</button><button onclick="pause();">Pause/Unpause</button>
</div>


<script language="JavaScript">

var player = null;
var paused = false;
var sampleFile = 'media/llama-drama-av1.webm';

player = new OGVPlayer({
	webGL: false
});
document.getElementById("videoarea").appendChild(player);

var vid = document.createElement('video');
vid.controls = true;
vid.playsInline = true;
document.getElementById("videoarea").appendChild(vid);

var captureStream;

player.addEventListener('loadedmetadata', function(_event) {
	console.log('loaded');
	var canvas = player.querySelector('canvas');
	captureStream = canvas.captureStream(0);
	if (!captureStream.requestFrame) {
		console.log('warning: pulling at 60fps because no requestFrame support');
		captureStream = canvas.captureStream(60);		
	}
	vid.srcObject = captureStream;
	vid.play();
	start().then(() => {
		call();
	}); // the webrtc hack
});

player.addEventListener('framecallback', function(_event) {
	if (captureStream && captureStream.requestFrame) {
		captureStream.requestFrame();
	}
});

function play(src) {
	//player = new OgvSwfPlayer();
	//player = document.createElement('video');
	player.src = src;
	player.play();	
}

function pause() {
	if(player) {
		if(!paused) {
			player.pause();
			paused = true;
		} else {
			player.play();
			paused = false;
		}
	}
}

function switchmedia(src) {
	sampleFile = src.value;
	play(sampleFile);
}






// from webrtc sample

let remoteVideo = document.createElement('video');
remoteVideo.controls = true;
remoteVideo.playsInline = true;
document.getElementById('videoarea').appendChild(remoteVideo);

let localStream;
let pc1;
let pc2;
const offerOptions = {
  offerToReceiveAudio: 1,
  offerToReceiveVideo: 1
};

function getName(pc) {
  return (pc === pc1) ? 'pc1' : 'pc2';
}

function getOtherPc(pc) {
  return (pc === pc1) ? pc2 : pc1;
}

async function start() {
  console.log('Requesting local stream');
  //startButton.disabled = true;
  try {
    //const stream = await navigator.mediaDevices.getUserMedia({audio: true, video: true});
	stream = captureStream;
    console.log('Received local stream');
    //localVideo.srcObject = stream;
	//localVideo.play();
    localStream = stream;
    //callButton.disabled = false;
  } catch (e) {
    alert(`error: ${e.name}`);
  }
}

function getSelectedSdpSemantics() {
  //const sdpSemanticsSelect = document.querySelector('#sdpSemantics');
  //const option = sdpSemanticsSelect.options[sdpSemanticsSelect.selectedIndex];
  const option = {value: ''};
  return option.value === '' ? {} : {sdpSemantics: option.value};
}

async function call() {
  //callButton.disabled = true;
  //hangupButton.disabled = false;
  console.log('Starting call');
  startTime = window.performance.now();
  const videoTracks = localStream.getVideoTracks();
  const audioTracks = localStream.getAudioTracks();
  if (videoTracks.length > 0) {
    console.log(`Using video device: ${videoTracks[0].label}`);
  }
  if (audioTracks.length > 0) {
    console.log(`Using audio device: ${audioTracks[0].label}`);
  }
  const configuration = getSelectedSdpSemantics();
  console.log('RTCPeerConnection configuration:', configuration);
  pc1 = new RTCPeerConnection(configuration);
  console.log('Created local peer connection object pc1');
  pc1.addEventListener('icecandidate', e => onIceCandidate(pc1, e));
  pc2 = new RTCPeerConnection(configuration);
  console.log('Created remote peer connection object pc2');
  pc2.addEventListener('icecandidate', e => onIceCandidate(pc2, e));
  pc1.addEventListener('iceconnectionstatechange', e => onIceStateChange(pc1, e));
  pc2.addEventListener('iceconnectionstatechange', e => onIceStateChange(pc2, e));
  pc2.addEventListener('track', gotRemoteStream);

  localStream.getTracks().forEach(track => pc1.addTrack(track, localStream));
  console.log('Added local stream to pc1');

  try {
    console.log('pc1 createOffer start');
    const offer = await pc1.createOffer(offerOptions);
    await onCreateOfferSuccess(offer);
  } catch (e) {
    onCreateSessionDescriptionError(e);
  }
}

function onCreateSessionDescriptionError(error) {
  console.log(`Failed to create session description: ${error.toString()}`);
}

async function onCreateOfferSuccess(desc) {
  console.log(`Offer from pc1\n${desc.sdp}`);
  console.log('pc1 setLocalDescription start');
  try {
    await pc1.setLocalDescription(desc);
    onSetLocalSuccess(pc1);
  } catch (e) {
    onSetSessionDescriptionError();
  }

  console.log('pc2 setRemoteDescription start');
  try {
    await pc2.setRemoteDescription(desc);
    onSetRemoteSuccess(pc2);
  } catch (e) {
    onSetSessionDescriptionError();
  }

  console.log('pc2 createAnswer start');
  // Since the 'remote' side has no media stream we need
  // to pass in the right constraints in order for it to
  // accept the incoming offer of audio and video.
  try {
    const answer = await pc2.createAnswer();
    await onCreateAnswerSuccess(answer);
  } catch (e) {
    onCreateSessionDescriptionError(e);
  }
}

function onSetLocalSuccess(pc) {
  console.log(`${getName(pc)} setLocalDescription complete`);
}

function onSetRemoteSuccess(pc) {
  console.log(`${getName(pc)} setRemoteDescription complete`);
}

function onSetSessionDescriptionError(error) {
  console.log(`Failed to set session description: ${error.toString()}`);
}

function gotRemoteStream(e) {
  if (remoteVideo.srcObject !== e.streams[0]) {
    remoteVideo.srcObject = e.streams[0];
    console.log('pc2 received remote stream');
	remoteVideo.play();
  }
}

async function onCreateAnswerSuccess(desc) {
  console.log(`Answer from pc2:\n${desc.sdp}`);
  console.log('pc2 setLocalDescription start');
  try {
    await pc2.setLocalDescription(desc);
    onSetLocalSuccess(pc2);
  } catch (e) {
    onSetSessionDescriptionError(e);
  }
  console.log('pc1 setRemoteDescription start');
  try {
    await pc1.setRemoteDescription(desc);
    onSetRemoteSuccess(pc1);
  } catch (e) {
    onSetSessionDescriptionError(e);
  }
}

async function onIceCandidate(pc, event) {
  try {
    await (getOtherPc(pc).addIceCandidate(event.candidate));
    onAddIceCandidateSuccess(pc);
  } catch (e) {
    onAddIceCandidateError(pc, e);
  }
  console.log(`${getName(pc)} ICE candidate:\n${event.candidate ? event.candidate.candidate : '(null)'}`);
}

function onAddIceCandidateSuccess(pc) {
  console.log(`${getName(pc)} addIceCandidate success`);
}

function onAddIceCandidateError(pc, error) {
  console.log(`${getName(pc)} failed to add ICE Candidate: ${error.toString()}`);
}

function onIceStateChange(pc, event) {
  if (pc) {
    console.log(`${getName(pc)} ICE state: ${pc.iceConnectionState}`);
    console.log('ICE state change event: ', event);
  }
}

function hangup() {
  console.log('Ending call');
  pc1.close();
  pc2.close();
  pc1 = null;
  pc2 = null;
  //hangupButton.disabled = true;
  //callButton.disabled = false;
}

</script>

</body></html>
